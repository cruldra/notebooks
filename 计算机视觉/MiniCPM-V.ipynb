{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- [简介](#toc1_)    \n",
    "- [安装](#toc2_)    \n",
    "  - [在`Windows 11`上安装](#toc2_1_)    \n",
    "    - [`CUDA`支持](#toc2_1_1_)    \n",
    "- [启动](#toc3_)    \n",
    "- [遇到的问题](#toc4_)    \n",
    "  - [flash_attn安装失败](#toc4_1_)    \n",
    "    - [问题描述](#toc4_1_1_)    \n",
    "    - [原因](#toc4_1_2_)    \n",
    "    - [解决方案](#toc4_1_3_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc1_'></a>[简介](#toc0_)\n",
    "\n",
    "[MiniCPM-V](https://github.com/OpenBMB/MiniCPM-V/blob/main/README_zh.md)是一个端侧可用的`GPT-4V`级开源单图、多图、视频多模态大模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc2_'></a>[安装](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc2_1_'></a>[在`Windows 11`上安装](#toc0_)\n",
    "\n",
    "```bash\n",
    "# 克隆代码\n",
    "git clone https://github.com/OpenBMB/MiniCPM-V.git\n",
    "\n",
    "# 创建 conda 环境\n",
    "conda create -n MiniCPMV python=3.10 -y\n",
    "\n",
    "# 激活 conda 环境\n",
    "conda activate MiniCPMV\n",
    "\n",
    "# 安装依赖\n",
    "pip install -r requirements.txt\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_1_1_'></a>[`CUDA`支持](#toc0_)\n",
    "\n",
    "使用`python web_demo_2.6.py --device cuda`启动时，需要安装`CUDA`支持\n",
    "\n",
    "1. 在[这里](https://pytorch.org/get-started/locally/)生成适用你的环境的安装命令,比如我这里是:\n",
    "\n",
    "```bash\n",
    "\n",
    "pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "\n",
    "```\n",
    "\n",
    "2. 安装完成后验证一下,运行下面的代码:\n",
    "\n",
    "```python\n",
    "import torch\n",
    "print(torch.__version__)\n",
    "# 2.1.2+cu118\n",
    "print(torch.cuda.is_available())\n",
    "# True\n",
    "print(torch.cuda.get_device_name(0))\n",
    "# NVIDIA GeForce RTX 2080 Ti\n",
    "```\n",
    "![image](https://github.com/cruldra/picx-images-hosting/raw/master/image.2a5093nwzg.png)\n",
    "\n",
    "**重要**: 需要升级到`CUDA12`以上的版本,否则安装不了`flash_attn`这个包导致无法运行\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc3_'></a>[启动](#toc0_)\n",
    "\n",
    "使用`python web_demo_2.6.py --device cuda`启动\n",
    "\n",
    "启动成功界面如下:\n",
    "\n",
    "![image](https://github.com/cruldra/picx-images-hosting/raw/master/image.1e8itumwiy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 运行\n",
    "\n",
    "在浏览器中输入上面的链接,`2.6`版本的演示界面如下:\n",
    "\n",
    "![image](https://github.com/cruldra/picx-images-hosting/raw/master/image.6f0llezksc.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc4_'></a>[遇到的问题](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc4_1_'></a>[flash_attn安装失败](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### <a id='toc4_1_1_'></a>[问题描述](#toc0_)\n",
    "\n",
    "一直卡在`Building wheel for flash_attn (setup.py) ...`这一步\n",
    "\n",
    "如图:\n",
    "\n",
    "![](https://github.com/cruldra/picx-images-hosting/raw/master/image.8dwsbqbh7q.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### <a id='toc4_1_2_'></a>[原因](#toc0_)\n",
    "\n",
    "在`Windows`上只能通过编译源码的形式安装`flash_attn`,需要下载`ninja`,会特别慢,参考[这篇文章](https://blog.csdn.net/lckj2009/article/details/136054392)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc4_1_3_'></a>[解决方案](#toc0_)\n",
    "\n",
    "[flash-attn windows环境安装踩坑日记](https://blog.csdn.net/2301_77818837/article/details/135642828)中提到到[这里](https://github.com/bdashore3/flash-attention/releases)去下载大神编译好的`whl`文件,然后使用`pip`安装\n",
    "\n",
    "```bash\n",
    "pip install flash_attn-2.6.3+cu123torch2.4.0cxx11abiFALSE-cp310-cp310-win_amd64.whl\n",
    "\n",
    "# flash_attn 2.6.3\n",
    "# torch 2.4.0\n",
    "# cu123\n",
    "# cp310\n",
    "```\n",
    "\n",
    "**注意**: 你需要根据你的`CUDA`版本和`torch`版本下载对应的`whl`文件"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".NET (C#)",
   "language": "C#",
   "name": ".net-csharp"
  },
  "language_info": {
   "name": "python"
  },
  "polyglot_notebook": {
   "kernelInfo": {
    "defaultKernelName": "csharp",
    "items": [
     {
      "aliases": [],
      "name": "csharp"
     }
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
